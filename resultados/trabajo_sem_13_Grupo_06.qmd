---
title: "Practica 4 - 1 de 3"
author: "GRUPO 06"
format: html
editor: visual
---

**GRUPO 6**

**INTEGRANTES:**

\- ALACOTE SALAS, Aylin Elisa.

\- ESQUIVEL MOLERO, Lourdes Valeria.

\- PALACIOS SALINAS. Danna Ariela.

\- ROJAS CORREA, Aitana.

\- VELASQUEZ ROMAN, Alvaro Fabian.

**Instalamos y cargamos los paquetes:**

```{r}
install.packages("factoextra")
install.packages("cluster")
```

```{r}
library(factoextra)
library(cluster)
library(here)
library(rio)
library(tidyverse)
```

**Importamos los datos:**

```{r}
cancer_data <- import(here("estadistica_upsjb_estudio/Estadistica/data/conoc_actit_factor_cancer_cervical.csv"))
```

**Preparamos los datos - Solo datos numéricos:**

```{r}
cancer_data_1 = cancer_data |> 
  select(-procedencia, -e_marital, -n_educacion, -religion, -etnia, -procedencia, -ocupacion, -ocupacion_convi, -antec_fam, -met_anticoncep, -antec_ets, -conocimiento, -actitud, -practica) |> 
  column_to_rownames("paciente_num")
```

**Standarizamos:**

```{r}
cancer_data_escalado = scale(cancer_data_1)
```

**Hechamos un vistazo a los datos antes del escalamiento:**

```{r}
head(cancer_data_1)
```

**Ahora con el escalado:**

```{r}
head(cancer_data_escalado)
```

**Calculo de distancias:**

```{r}
dist_cancer_data <- dist(cancer_data_escalado, method = "euclidean")
```

**Visualizamos las distancias euclidians en un mapa de calor:**

```{r}
fviz_dist(dist_cancer_data)
```

Nuestro mapa de calor donde se visualiza la matriz de distancias euclidianas de nuestras muestras de datos de "dist_cancer_data". Los tonos más claros (naranja/blanco) indican distancias menores (mayor similitud), mientras que los tonos más oscuros (violeta/azul) representan distancias mayores (menor similitud). Se observa una estructura de bloques, sugiriendo la **presencia de clústeres o agrupaciones naturales dentro de los datos,** donde **las muestras dentro de un bloque son más similares entre sí que con las muestras de otros bloques.**

**Usamos el metodo de agrupamiento: Funcion de enlace (linkage)**

```{r}
dist_link_cancer_data <- hclust(d = dist_cancer_data, method = "ward.D2")
```

**Empleamos los dendogramas par la visualizacion de los patrones:**

```{r}
fviz_dend(dist_link_cancer_data, cex = 0.7)
```

En nuestro dendrograma, vemos la representación gráfica del análisis de clúster jerárquico. En el eje Y, "Height", indica la distancia o disimilitud a la que los clústeres se fusionan. Las ramas que se unen a alturas bajas representan muestras o grupos de muestras muy similares. A medida que se asciende en el dendrograma, las uniones ocurren a mayores distancias, indicando la formación de clústeres más grandes y heterogéneos. La estructura ramificada revela la jerarquía de las agrupaciones, permitiendo identificar subgrupos dentro de los datos.

**Revisamos los grupos que se formaron en el dendograma:**

```{r}
fviz_dend(dist_link_cancer_data, 
          k = 3,
          cex = 0.5,
          k_colors = c("#e700a9", "#0ce700", "#e70400"),
          color_labels_by_k = TRUE, 
          rect = TRUE)
```

Se muestran los resultados de un análisis de clúster jerárquico, donde se han identificado y visualizado K=3 grupos distintos, según la instrucción del código (k=3). Cada color (magenta, verde, rojo) representa uno de los tres clústeres. Las líneas punteadas verticales indican los límites de estos clústeres. La "Height" en el eje Y sigue representando la disimilitud. La segmentación visual facilita la interpretación de las agrupaciones de las muestras, revelando patrones y relaciones inherentes dentro del conjunto de datos.

**Estimamos el numero optimo de clusters:**

1° escalamos los datos:

```{r}
cancer_data_escalado = scale(cancer_data_1)
```

2° Graficamos la suma de cuadrados dentro de los graficos:

```{r}
fviz_nbclust(cancer_data_escalado, kmeans, nstart = 25, method = "wss") + 
  geom_vline(xintercept = 3, linetype = 2)
```

Se muestra la "Total Within Sum of Square" (WCSS) en función del número de clústeres (k). Se busca el punto de inflexión, o "codo", donde la disminución marginal de WCSS se estanca. La línea punteada indica k=3 como el número óptimo, sugiriendo que más de tres clústeres no aportan una reducción significativa en la varianza intra-clúster.

**Calculamos el agrupamiento de los k-means:**

```{r}
set.seed(123)
km_res <- kmeans(cancer_data_escalado, 3, nstart = 25)
```

```{r}
km_res
```

**Visualizamos los clusters k-means**

```{r}
fviz_cluster(
  km_res,
  data = cancer_data_escalado,
  palette = c("#0064e7", "#e70000", "#1be700"),
  ellipse.type = "euclid",
  repel = TRUE,
  ggtheme = theme_minimal()
)
```

En nuestro gráfico bidimensional se ilustra la asignación de **muestras de datos a tres clústeres distintos** tras un algoritmo de K-means. Cada color (azul, rojo, verde) denota un clúster. La dispersión elíptica alrededor de cada centroide, basada en la distancia euclidiana, visualiza la variabilidad intraclúster. La clara separación espacial entre los grupos valida la efectividad del clustering, revelando **agrupaciones naturales** en el dataset proyectado.

**CONCLUSION:**

Las visualizaciones como mapas de calor y dendrogramas nos ofrecieron una inspección inicial de las relaciones de similitud en los datos, insinuando la presencia de estructuras de grupo, donde el método del codo nos sirvios para determinar el número óptimo de conglomerados, el cual nos ayudo para guiarnos la aplicación efectiva de algoritmos de Machine Learning como K-means. Los resultados numéricos de las agrupaciones, junto con los gráficos de clústeres, confirman la coherencia de los segmentos identificados. Esta aplicación de técnicas de clustering fue importante para descubrir patrones intrínsecos y facilitar la comprensión de datasets complejos con relacion a los valores mas que nada numericos de nuestra data con respecto al cancer cervical con relacion a la "edad", "edad relacion sexual", "parejas sexuales" y "numero de hijos"
